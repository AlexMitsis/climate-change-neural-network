{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexMitsis/climate-change-neural-network/blob/main/Neural_Networks_Project_(EN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Intelligent Systems Project**\n",
        "\n",
        "Regression with neural networks and the climate change dataset."
      ],
      "metadata": {
        "id": "seu95T5hvb_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Loading and preparation**\n",
        "\n",
        "Loading libraries, loading and displaying data"
      ],
      "metadata": {
        "id": "iIJJzcqWvyeg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDk2kAq9s-9Y"
      },
      "outputs": [],
      "source": [
        "# We import all the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We load the CSV file with the Pandas library for display purposes only\n",
        "dataframe = pd.read_csv('climate_change.csv')\n",
        "print(dataframe.head(5))\n",
        "print(\"Dimensions:\", dataframe.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgvJhpZFIGlT",
        "outputId": "ab411298-b027-476b-c063-d294f621d9cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Year  Month    MEI     CO2      CH4      N2O   CFC-11   CFC-12        TSI  \\\n",
            "0  1983      5  2.556  345.96  1638.59  303.677  191.324  350.113  1366.1024   \n",
            "1  1983      6  2.167  345.52  1633.71  303.746  192.057  351.848  1366.1208   \n",
            "2  1983      7  1.741  344.15  1633.22  303.795  192.818  353.725  1366.2850   \n",
            "3  1983      8  1.130  342.25  1631.35  303.839  193.602  355.633  1366.4202   \n",
            "4  1983      9  0.428  340.17  1648.40  303.901  194.392  357.465  1366.2335   \n",
            "\n",
            "   Aerosols   Temp  \n",
            "0    0.0863  0.109  \n",
            "1    0.0794  0.118  \n",
            "2    0.0731  0.137  \n",
            "3    0.0673  0.176  \n",
            "4    0.0619  0.149  \n",
            "Διαστάσεις: (308, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creation of a custom Dataset object\n",
        "\n",
        "We create a Dataset object that will feed the dataloaders, and a transform object that will modify the data before it is read by the loaders.\n",
        "\n",
        "Transform is very important for data augmentation, but in this case we use it for data normalization."
      ],
      "metadata": {
        "id": "Fd9abT0PwEbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClimateDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, csv_file, transform=None, train=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      csv_file (string): Path to the csv file\n",
        "      transform (callable, optional): Optional transform to be applied\n",
        "          on each sample.\n",
        "      train (bool): whether to create the training set or the test set\n",
        "    \"\"\"\n",
        "\n",
        "    # We define the size of the training set\n",
        "    train_set_size = 250\n",
        "\n",
        "    # Loading the csv via Pandas, deleting columns that do not help in the analysis,\n",
        "    # converting Pandas Dataframes to Numpy Arrays, which have more helpful\n",
        "    # properties\n",
        "    dataframe = pd.read_csv(csv_file)\n",
        "    targets = np.array(dataframe[['Temp']]).astype('float32')\n",
        "    data = np.array(dataframe.drop(columns=['Year','Month','Temp'])).astype('float32')\n",
        "\n",
        "    # Separation of data into training and test set\n",
        "    # This line will have to be replaced in the future if we want to do\n",
        "    # more realistic evaluation\n",
        "    # We also set the seed to random state for repeatability\n",
        "    train_data, test_data, train_targets, test_targets = train_test_split(data,\n",
        "                            targets, train_size=train_set_size, random_state=665)\n",
        "\n",
        "\n",
        "    # We set and initialize the transform\n",
        "    self.transform = transform\n",
        "    # Attention, the transform is initialized with the values of the training set, either\n",
        "    # the dataset refers to the train or the test set.\n",
        "    self.transform.fit(train_data, train_targets)\n",
        "\n",
        "    # If we have been asked to create a train database, we keep the train data.\n",
        "    # Otherwise we keep the test data\n",
        "    if train==True:\n",
        "      self.data = train_data\n",
        "      self.targets = train_targets\n",
        "    else:\n",
        "      self.data = test_data\n",
        "      self.targets = test_targets\n",
        "    # The initialisation ends here\n",
        "\n",
        "  # The __len__ of a Dataset object must return the number of\n",
        "  # of its objects.\n",
        "  def __len__(self):\n",
        "    return len(self.targets)\n",
        "\n",
        "  # The __getitem__ of a Dataset object takes as parameter an index and\n",
        "  # returns the corresponding object. It is used by the DataLoader to\n",
        "  # derive minibatches.\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "    item_data = self.data[idx,:]\n",
        "    item_target = self.targets[idx,:]\n",
        "\n",
        "    # If we have a transform defined, we apply it to the object before\n",
        "    # return. When I call transform by its name\n",
        "    # transform.__call__() is called (see below)\n",
        "    if self.transform:\n",
        "      item_data, item_target = self.transform(item_data, item_target)\n",
        "    return item_data, item_target\n",
        "\n",
        "\n",
        "  # This class will play the role of Transform. It will remove the minimum value\n",
        "  # from each column of data and divide by its range to normalize\n",
        "  # the values of all variables (and targets) to [0,1]\n",
        "  class MinMaxScaler():\n",
        "\n",
        "  # The fit is initialized with the training data, and calculates the minimum and\n",
        "  # maximum values. The objects are numpy arrays,\n",
        "  # so they have the min and max function built-in\n",
        "  def fit(self, data, targets):\n",
        "    self.data_min = data.min(0, keepdims=True)\n",
        "    self.target_min = targets.min(0, keepdims=True)\n",
        "    self.data_max = data.max(0, keepdims=True)\n",
        "    self.target_max = targets.max(0, keepdims=True)\n",
        "    return self\n",
        "\n",
        "  # When transform.__call__() is called, it accepts an object (data and target)\n",
        "  # and returns it transformed - in this case normalized\n",
        "  def __call__(self, data, target):\n",
        "    data = (data - self.data_min)/(self.data_max-self.data_min)\n",
        "    target = (target - self.target_min)/(self.target_max-self.target_min)\n",
        "    return data, target\n",
        "\n",
        "  # We'll train a system to learn the normalized targets.\n",
        "  # For real-world application, we'll need to be able to transform\n",
        "  # the outputs of the system back to its original range of values.\n",
        "  def inverse_transform(self, data, target):\n",
        "    data=data * (self.data_max-self.data_min) + self.data_min\n",
        "    target = target * (self.target_max-self.target_min) + self.target_min\n",
        "    return data, target"
      ],
      "metadata": {
        "id": "1y_r_4ngKxAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creation of DataLoaders\n",
        "\n",
        "We create dataloaders for the training set and for the test set."
      ],
      "metadata": {
        "id": "SUa7z3yH0t5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The batch size is big enough to fit all the training data and all the\n",
        "# all the test data in one loop (practically we apply Batch Gradient Descent, each\n",
        "# Batch is an Epoch)\n",
        "# (if the batch size is larger than the available data, pytorch\n",
        "# creates a batch with the available data)\n",
        "batch_size = 500\n",
        "transform=MinMaxScaler()\n",
        "\n",
        "train_dataset = ClimateDataset(csv_file='climate_change.csv', train=True, transform=transform)\n",
        "test_dataset = ClimateDataset(csv_file='climate_change.csv', train=False, transform=transform)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "YQmvU0I50tBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network architecture\n",
        "\n",
        "We define a simple network with one neuron for linear regression"
      ],
      "metadata": {
        "id": "doGIwzMX1FP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Network defining code\n",
        "class NeuralNetwork1(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "      # I run the init of the parent class\n",
        "      super(NeuralNetwork1, self).__init__()\n",
        "      # The input x of the network consists of 8 types of variables, after removing the columns with year, month and temperature\n",
        "      self.network_architecture_linear = nn.Linear(8, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "      out = self.network_architecture_linear(x)\n",
        "      return out"
      ],
      "metadata": {
        "id": "SWZzmPr01XuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Test Loop\n",
        "\n",
        "We write the code for the training loop and the test loop"
      ],
      "metadata": {
        "id": "CSZJ-_-K1YRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer, current_epoch):\n",
        "    size = len(dataloader.dataset)\n",
        "    # enumerate(dataloader) returns the number of the batch,\n",
        "    # and the batch itself i.e. a number of Tensors X and targets y equal to batch_size\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # We show the loss every 20 epochs\n",
        "        if((current_epoch+1)%20==0):\n",
        "            loss = loss.item()\n",
        "            print(f\"loss: {loss:>7f} (Epoch: {current_epoch+1})\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn, current_epoch):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            # convert data to the appropriate format for r2_score\n",
        "            pred = model(X).permute(1,0,2).reshape(58,1)\n",
        "            y = y.permute(1,0,2).reshape(58,1)\n",
        "            r2 = r2_score(y, pred)\n",
        "            # Item() converts the Tensor to a simple number\n",
        "            test_loss = loss_fn(pred, y).item()\n",
        "\n",
        "\n",
        "    # R2 score values from 0 to 1 express the percentage of the variance of the data that is correctly expressed by the model.\n",
        "    # Values below 0 mean that the model is worse than an estimator that simply always predicts the average value of the training set.\n",
        "\n",
        "    print(f\"\\nTest Error for Epoch {current_epoch+1}: \\nR2 score: {r2:>0.2f}, Avg loss: {test_loss:>8f} \\n\\n\")\n"
      ],
      "metadata": {
        "id": "ffBgVWDpbVef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network 1:\n",
        "\n",
        "As each epoch consists of 1 batch I set the loss to be displayed every 20 epochs\n",
        "while test_loop is called every 100 epochs\n",
        "\n",
        "As the epochs pass we observe that the train/test losses decrease\n",
        "while the R2 score increases. Logically 3000 epochs is not enough time to train the model since the R2 score reaches a value close to 0.3 (ideally it would be closer to 1 than 0). Indeed, I have experimentally confirmed that for more epochs there is a large room for improvement of our model. (increase of R2 score, decrease of loss)\n",
        "\n",
        "By changing torch.manual_seed(50) to torch.manual_seed(100) we see a slight deterioration in our results. It may be that with 100 instead of 50 the random initialized weights are farther away from the weights we ideally want our model to have. For 50 it takes 14 seconds to complete while for 100 it takes 12 seconds.\n",
        "\n",
        "By replacing the optimizer from SGD to Adam it seems that we generally have a better R2 score and less loss in our data. The R2 score continues to grow more steeply than when we used SGD until it reaches a point where it stays somewhat constant. Similarly the test loss decreases more steeply per epoch. For some reason this small increase is not seen in the train loss, where it continues to decrease steadily but at a slower rate after the steep increase. There is a possibility that the final results are better for SGD if we run the model for more epochs, but it seems that Adam is much better if we have limited time available (in this case 3000 epochs).\n",
        "\n",
        "## Neural Network 2:\n",
        "The output is not linear in this case due to the hidden layers on which we have applied the non-linear activation function ReLU.\n",
        "\n",
        "With SGD we train the model in 13 seconds and with Adam in 15 seconds.\n",
        "\n",
        "With SGD we observe as expected that the R2 score increases while the train/test loss decreases with the passage of epochs. With Adam we observe an increase in R2 score and decrease in test loss until around epoch 400 where after that the opposite paradoxically starts to happen (decrease in R2 score, increase in loss). The train loss however continues to decrease normally after this point. Finally Adam gives us better results even though his step performance gets worse after epoch 400.\n",
        "\n",
        "We notice that the test error doesn't improve if we keep running the script for a long time.\n",
        "The phenomenon of having low Training Error but high Test Error is the definition of overfitting. In neural networks because training is gradual over time, this phenomenon can be observed to evolve over time. This is why we have the practice of Early Stopping, where we achieve regularization by stopping training before the Test Error starts to drop (the task did not call for Early Stopping)."
      ],
      "metadata": {
        "id": "RF_TfBa89VF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter definition and network training\n",
        "\n",
        "Network execution and evaluation code"
      ],
      "metadata": {
        "id": "pjPSgch1sf7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NeuralNetwork2\n"
      ],
      "metadata": {
        "id": "RtRISRUYmobi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Code definition of the new network\n",
        "\n",
        "class NeuralNetwork2(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "      # Run the init of the parent class\n",
        "      super(NeuralNetwork2, self).__init__()\n",
        "      # the input x of the network consists of 8 types of variables, after removing the columns with year, month and temperature\n",
        "\n",
        "      self.network_architecture = nn.Sequential(\n",
        "            nn.Linear(8, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 1),\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "      out = self.network_architecture(x)\n",
        "      return out\n"
      ],
      "metadata": {
        "id": "nUebEAOrll20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# manual_seed determines the random initialization of the weights/network parameters\n",
        "# It is just a fixed seed in pytorch's random number generator.\n",
        "torch.manual_seed(50)\n",
        "#torch.manual_seed(100)\n",
        "\n",
        "model = NeuralNetwork1()\n",
        "#model = NeuralNetwork2()\n",
        "\n",
        "# appropriate loss function for regression problems\n",
        "loss_fn = nn.MSELoss()\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# We set the optimizer to use the SGD algorithm\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "# Replace SGD with Adam\n",
        "optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
        "\n",
        "# Training code here:\n",
        "epochs = 3000\n",
        "epoch_step = 100\n",
        "for t in range(epochs):\n",
        "    if(t==0):\n",
        "      print(f\"Epoch {t+1} - {t+epoch_step}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer, t)\n",
        "    if((t+1)%100==0):\n",
        "      test_loop(test_dataloader, model, loss_fn, t)\n",
        "    if((t+1)%100==0 and (t+1)!=epochs):\n",
        "      print(f\"Epoch {t+1} - {t+1+epoch_step}\\n-------------------------------\")\n",
        "print(\"Done!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "f1beitvisMNY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "775a704d-8590-46c3-e549-895176a609d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - 100\n",
            "-------------------------------\n",
            "loss: 0.922417 (Epoch: 20)\n",
            "loss: 0.710118 (Epoch: 40)\n",
            "loss: 0.549098 (Epoch: 60)\n",
            "loss: 0.426950 (Epoch: 80)\n",
            "loss: 0.334270 (Epoch: 100)\n",
            "\n",
            "Test Error for Epoch 100: \n",
            "R2 score: -20.14, Avg loss: 0.443976 \n",
            "\n",
            "\n",
            "Epoch 100 - 200\n",
            "-------------------------------\n",
            "loss: 0.263929 (Epoch: 120)\n",
            "loss: 0.210523 (Epoch: 140)\n",
            "loss: 0.169955 (Epoch: 160)\n",
            "loss: 0.139120 (Epoch: 180)\n",
            "loss: 0.115664 (Epoch: 200)\n",
            "\n",
            "Test Error for Epoch 200: \n",
            "R2 score: -6.72, Avg loss: 0.162051 \n",
            "\n",
            "\n",
            "Epoch 200 - 300\n",
            "-------------------------------\n",
            "loss: 0.097803 (Epoch: 220)\n",
            "loss: 0.084183 (Epoch: 240)\n",
            "loss: 0.073780 (Epoch: 260)\n",
            "loss: 0.065816 (Epoch: 280)\n",
            "loss: 0.059702 (Epoch: 300)\n",
            "\n",
            "Test Error for Epoch 300: \n",
            "R2 score: -2.76, Avg loss: 0.078962 \n",
            "\n",
            "\n",
            "Epoch 300 - 400\n",
            "-------------------------------\n",
            "loss: 0.054991 (Epoch: 320)\n",
            "loss: 0.051344 (Epoch: 340)\n",
            "loss: 0.048505 (Epoch: 360)\n",
            "loss: 0.046280 (Epoch: 380)\n",
            "loss: 0.044520 (Epoch: 400)\n",
            "\n",
            "Test Error for Epoch 400: \n",
            "R2 score: -1.45, Avg loss: 0.051431 \n",
            "\n",
            "\n",
            "Epoch 400 - 500\n",
            "-------------------------------\n",
            "loss: 0.043114 (Epoch: 420)\n",
            "loss: 0.041977 (Epoch: 440)\n",
            "loss: 0.041045 (Epoch: 460)\n",
            "loss: 0.040269 (Epoch: 480)\n",
            "loss: 0.039612 (Epoch: 500)\n",
            "\n",
            "Test Error for Epoch 500: \n",
            "R2 score: -0.94, Avg loss: 0.040680 \n",
            "\n",
            "\n",
            "Epoch 500 - 600\n",
            "-------------------------------\n",
            "loss: 0.039046 (Epoch: 520)\n",
            "loss: 0.038550 (Epoch: 540)\n",
            "loss: 0.038107 (Epoch: 560)\n",
            "loss: 0.037706 (Epoch: 580)\n",
            "loss: 0.037337 (Epoch: 600)\n",
            "\n",
            "Test Error for Epoch 600: \n",
            "R2 score: -0.69, Avg loss: 0.035569 \n",
            "\n",
            "\n",
            "Epoch 600 - 700\n",
            "-------------------------------\n",
            "loss: 0.036994 (Epoch: 620)\n",
            "loss: 0.036670 (Epoch: 640)\n",
            "loss: 0.036362 (Epoch: 660)\n",
            "loss: 0.036066 (Epoch: 680)\n",
            "loss: 0.035781 (Epoch: 700)\n",
            "\n",
            "Test Error for Epoch 700: \n",
            "R2 score: -0.55, Avg loss: 0.032607 \n",
            "\n",
            "\n",
            "Epoch 700 - 800\n",
            "-------------------------------\n",
            "loss: 0.035504 (Epoch: 720)\n",
            "loss: 0.035235 (Epoch: 740)\n",
            "loss: 0.034971 (Epoch: 760)\n",
            "loss: 0.034713 (Epoch: 780)\n",
            "loss: 0.034459 (Epoch: 800)\n",
            "\n",
            "Test Error for Epoch 800: \n",
            "R2 score: -0.46, Avg loss: 0.030573 \n",
            "\n",
            "\n",
            "Epoch 800 - 900\n",
            "-------------------------------\n",
            "loss: 0.034210 (Epoch: 820)\n",
            "loss: 0.033965 (Epoch: 840)\n",
            "loss: 0.033723 (Epoch: 860)\n",
            "loss: 0.033484 (Epoch: 880)\n",
            "loss: 0.033249 (Epoch: 900)\n",
            "\n",
            "Test Error for Epoch 900: \n",
            "R2 score: -0.38, Avg loss: 0.028990 \n",
            "\n",
            "\n",
            "Epoch 900 - 1000\n",
            "-------------------------------\n",
            "loss: 0.033016 (Epoch: 920)\n",
            "loss: 0.032787 (Epoch: 940)\n",
            "loss: 0.032560 (Epoch: 960)\n",
            "loss: 0.032336 (Epoch: 980)\n",
            "loss: 0.032115 (Epoch: 1000)\n",
            "\n",
            "Test Error for Epoch 1000: \n",
            "R2 score: -0.32, Avg loss: 0.027651 \n",
            "\n",
            "\n",
            "Epoch 1000 - 1100\n",
            "-------------------------------\n",
            "loss: 0.031896 (Epoch: 1020)\n",
            "loss: 0.031680 (Epoch: 1040)\n",
            "loss: 0.031466 (Epoch: 1060)\n",
            "loss: 0.031255 (Epoch: 1080)\n",
            "loss: 0.031046 (Epoch: 1100)\n",
            "\n",
            "Test Error for Epoch 1100: \n",
            "R2 score: -0.26, Avg loss: 0.026463 \n",
            "\n",
            "\n",
            "Epoch 1100 - 1200\n",
            "-------------------------------\n",
            "loss: 0.030840 (Epoch: 1120)\n",
            "loss: 0.030636 (Epoch: 1140)\n",
            "loss: 0.030434 (Epoch: 1160)\n",
            "loss: 0.030234 (Epoch: 1180)\n",
            "loss: 0.030037 (Epoch: 1200)\n",
            "\n",
            "Test Error for Epoch 1200: \n",
            "R2 score: -0.21, Avg loss: 0.025378 \n",
            "\n",
            "\n",
            "Epoch 1200 - 1300\n",
            "-------------------------------\n",
            "loss: 0.029842 (Epoch: 1220)\n",
            "loss: 0.029650 (Epoch: 1240)\n",
            "loss: 0.029459 (Epoch: 1260)\n",
            "loss: 0.029271 (Epoch: 1280)\n",
            "loss: 0.029084 (Epoch: 1300)\n",
            "\n",
            "Test Error for Epoch 1300: \n",
            "R2 score: -0.16, Avg loss: 0.024373 \n",
            "\n",
            "\n",
            "Epoch 1300 - 1400\n",
            "-------------------------------\n",
            "loss: 0.028900 (Epoch: 1320)\n",
            "loss: 0.028718 (Epoch: 1340)\n",
            "loss: 0.028538 (Epoch: 1360)\n",
            "loss: 0.028360 (Epoch: 1380)\n",
            "loss: 0.028184 (Epoch: 1400)\n",
            "\n",
            "Test Error for Epoch 1400: \n",
            "R2 score: -0.12, Avg loss: 0.023434 \n",
            "\n",
            "\n",
            "Epoch 1400 - 1500\n",
            "-------------------------------\n",
            "loss: 0.028010 (Epoch: 1420)\n",
            "loss: 0.027838 (Epoch: 1440)\n",
            "loss: 0.027668 (Epoch: 1460)\n",
            "loss: 0.027500 (Epoch: 1480)\n",
            "loss: 0.027333 (Epoch: 1500)\n",
            "\n",
            "Test Error for Epoch 1500: \n",
            "R2 score: -0.07, Avg loss: 0.022552 \n",
            "\n",
            "\n",
            "Epoch 1500 - 1600\n",
            "-------------------------------\n",
            "loss: 0.027169 (Epoch: 1520)\n",
            "loss: 0.027006 (Epoch: 1540)\n",
            "loss: 0.026845 (Epoch: 1560)\n",
            "loss: 0.026686 (Epoch: 1580)\n",
            "loss: 0.026529 (Epoch: 1600)\n",
            "\n",
            "Test Error for Epoch 1600: \n",
            "R2 score: -0.03, Avg loss: 0.021723 \n",
            "\n",
            "\n",
            "Epoch 1600 - 1700\n",
            "-------------------------------\n",
            "loss: 0.026374 (Epoch: 1620)\n",
            "loss: 0.026220 (Epoch: 1640)\n",
            "loss: 0.026068 (Epoch: 1660)\n",
            "loss: 0.025918 (Epoch: 1680)\n",
            "loss: 0.025769 (Epoch: 1700)\n",
            "\n",
            "Test Error for Epoch 1700: \n",
            "R2 score: 0.00, Avg loss: 0.020942 \n",
            "\n",
            "\n",
            "Epoch 1700 - 1800\n",
            "-------------------------------\n",
            "loss: 0.025622 (Epoch: 1720)\n",
            "loss: 0.025477 (Epoch: 1740)\n",
            "loss: 0.025333 (Epoch: 1760)\n",
            "loss: 0.025191 (Epoch: 1780)\n",
            "loss: 0.025050 (Epoch: 1800)\n",
            "\n",
            "Test Error for Epoch 1800: \n",
            "R2 score: 0.04, Avg loss: 0.020207 \n",
            "\n",
            "\n",
            "Epoch 1800 - 1900\n",
            "-------------------------------\n",
            "loss: 0.024911 (Epoch: 1820)\n",
            "loss: 0.024774 (Epoch: 1840)\n",
            "loss: 0.024638 (Epoch: 1860)\n",
            "loss: 0.024504 (Epoch: 1880)\n",
            "loss: 0.024371 (Epoch: 1900)\n",
            "\n",
            "Test Error for Epoch 1900: \n",
            "R2 score: 0.07, Avg loss: 0.019513 \n",
            "\n",
            "\n",
            "Epoch 1900 - 2000\n",
            "-------------------------------\n",
            "loss: 0.024239 (Epoch: 1920)\n",
            "loss: 0.024109 (Epoch: 1940)\n",
            "loss: 0.023981 (Epoch: 1960)\n",
            "loss: 0.023853 (Epoch: 1980)\n",
            "loss: 0.023728 (Epoch: 2000)\n",
            "\n",
            "Test Error for Epoch 2000: \n",
            "R2 score: 0.10, Avg loss: 0.018858 \n",
            "\n",
            "\n",
            "Epoch 2000 - 2100\n",
            "-------------------------------\n",
            "loss: 0.023603 (Epoch: 2020)\n",
            "loss: 0.023480 (Epoch: 2040)\n",
            "loss: 0.023359 (Epoch: 2060)\n",
            "loss: 0.023238 (Epoch: 2080)\n",
            "loss: 0.023119 (Epoch: 2100)\n",
            "\n",
            "Test Error for Epoch 2100: \n",
            "R2 score: 0.13, Avg loss: 0.018241 \n",
            "\n",
            "\n",
            "Epoch 2100 - 2200\n",
            "-------------------------------\n",
            "loss: 0.023002 (Epoch: 2120)\n",
            "loss: 0.022885 (Epoch: 2140)\n",
            "loss: 0.022770 (Epoch: 2160)\n",
            "loss: 0.022656 (Epoch: 2180)\n",
            "loss: 0.022544 (Epoch: 2200)\n",
            "\n",
            "Test Error for Epoch 2200: \n",
            "R2 score: 0.16, Avg loss: 0.017659 \n",
            "\n",
            "\n",
            "Epoch 2200 - 2300\n",
            "-------------------------------\n",
            "loss: 0.022432 (Epoch: 2220)\n",
            "loss: 0.022322 (Epoch: 2240)\n",
            "loss: 0.022213 (Epoch: 2260)\n",
            "loss: 0.022106 (Epoch: 2280)\n",
            "loss: 0.021999 (Epoch: 2300)\n",
            "\n",
            "Test Error for Epoch 2300: \n",
            "R2 score: 0.19, Avg loss: 0.017109 \n",
            "\n",
            "\n",
            "Epoch 2300 - 2400\n",
            "-------------------------------\n",
            "loss: 0.021894 (Epoch: 2320)\n",
            "loss: 0.021789 (Epoch: 2340)\n",
            "loss: 0.021686 (Epoch: 2360)\n",
            "loss: 0.021584 (Epoch: 2380)\n",
            "loss: 0.021483 (Epoch: 2400)\n",
            "\n",
            "Test Error for Epoch 2400: \n",
            "R2 score: 0.21, Avg loss: 0.016590 \n",
            "\n",
            "\n",
            "Epoch 2400 - 2500\n",
            "-------------------------------\n",
            "loss: 0.021383 (Epoch: 2420)\n",
            "loss: 0.021285 (Epoch: 2440)\n",
            "loss: 0.021187 (Epoch: 2460)\n",
            "loss: 0.021090 (Epoch: 2480)\n",
            "loss: 0.020995 (Epoch: 2500)\n",
            "\n",
            "Test Error for Epoch 2500: \n",
            "R2 score: 0.23, Avg loss: 0.016101 \n",
            "\n",
            "\n",
            "Epoch 2500 - 2600\n",
            "-------------------------------\n",
            "loss: 0.020900 (Epoch: 2520)\n",
            "loss: 0.020807 (Epoch: 2540)\n",
            "loss: 0.020714 (Epoch: 2560)\n",
            "loss: 0.020623 (Epoch: 2580)\n",
            "loss: 0.020532 (Epoch: 2600)\n",
            "\n",
            "Test Error for Epoch 2600: \n",
            "R2 score: 0.26, Avg loss: 0.015639 \n",
            "\n",
            "\n",
            "Epoch 2600 - 2700\n",
            "-------------------------------\n",
            "loss: 0.020443 (Epoch: 2620)\n",
            "loss: 0.020354 (Epoch: 2640)\n",
            "loss: 0.020266 (Epoch: 2660)\n",
            "loss: 0.020180 (Epoch: 2680)\n",
            "loss: 0.020094 (Epoch: 2700)\n",
            "\n",
            "Test Error for Epoch 2700: \n",
            "R2 score: 0.28, Avg loss: 0.015202 \n",
            "\n",
            "\n",
            "Epoch 2700 - 2800\n",
            "-------------------------------\n",
            "loss: 0.020009 (Epoch: 2720)\n",
            "loss: 0.019925 (Epoch: 2740)\n",
            "loss: 0.019842 (Epoch: 2760)\n",
            "loss: 0.019760 (Epoch: 2780)\n",
            "loss: 0.019679 (Epoch: 2800)\n",
            "\n",
            "Test Error for Epoch 2800: \n",
            "R2 score: 0.30, Avg loss: 0.014790 \n",
            "\n",
            "\n",
            "Epoch 2800 - 2900\n",
            "-------------------------------\n",
            "loss: 0.019598 (Epoch: 2820)\n",
            "loss: 0.019519 (Epoch: 2840)\n",
            "loss: 0.019440 (Epoch: 2860)\n",
            "loss: 0.019362 (Epoch: 2880)\n",
            "loss: 0.019285 (Epoch: 2900)\n",
            "\n",
            "Test Error for Epoch 2900: \n",
            "R2 score: 0.31, Avg loss: 0.014401 \n",
            "\n",
            "\n",
            "Epoch 2900 - 3000\n",
            "-------------------------------\n",
            "loss: 0.019209 (Epoch: 2920)\n",
            "loss: 0.019133 (Epoch: 2940)\n",
            "loss: 0.019059 (Epoch: 2960)\n",
            "loss: 0.018985 (Epoch: 2980)\n",
            "loss: 0.018912 (Epoch: 3000)\n",
            "\n",
            "Test Error for Epoch 3000: \n",
            "R2 score: 0.33, Avg loss: 0.014033 \n",
            "\n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    }
  ]
}